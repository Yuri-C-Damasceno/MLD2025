{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=\"100%\">\n",
    "    <td align=\"left\">\n",
    "        <a target=\"_blank\", href=\"https://www.up.pt/fcup/en/\">\n",
    "            <img src=\"https://divulgacao.iastro.pt/wp-content/uploads/2023/03/FCUP_logo-print_blcktransp_600ppi.png\" width=\"90px\" height=\"90px\" style=\"padding-bottom:5px;\"/>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td>\n",
    "        <a target=\"_blank\", href=\"https://www.iastro.pt/\">\n",
    "            <img src=\"https://divulgacao.iastro.pt/wp-content/uploads/2018/03/IA_logo_bitmap-rgbblack-1200px-388x259.png\" width=\"90px\" height=\"90px\" style=\"padding-bottom:5px;\"/>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/jbrinchmann/MLD2025/blob/main/Notebooks/MLD2025-01b-Deciding%20when%20good%20is%20good%20enough.ipynb\">\n",
    "           <img src=\"https://tinyurl.com/3mm2cyk6\"  width=\"90px\" height=\"90px\" style=\"padding-bottom:5px;\"/>Run in Google Colab\n",
    "        </a>\n",
    "    </td>\n",
    "<td align=\"center\"><a target=\"_blank\" href=\"https://github.com/jbrinchmann/MLD2025/blob/main/Notebooks/MLD2025-01b-Deciding%20when%20good%20is%20good%20enough.ipynb\">\n",
    "<img src=\"https://tinyurl.com/25h5fw53\"  width=\"90px\" height=\"60px\" style=\"padding-bottom:0px;\"  />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When is good good enough?\n",
    "\n",
    "In the previous notebook we looked at the quality of fit and we concluded with the result that the best fit to the data we had created was provided by a polynomial of very high degree. This does not (should not!) match with our intuition but mathematically it makes sense. So we clearly need to take a different approach. \n",
    "\n",
    "In this notebook we will therefore look at train-test-validation splits of your data. This is actually a rather complex topic in general so we will do it step-by-step. We start out with a simple extension - namely to split our data simply into a test and a training set, then we will look at training-test-validation split and finally at cross-validation.\n",
    "\n",
    "For the first part we will use the same polynomial function fits - I have stored it in a python file since it is not important for the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# Get the Huber loss function\n",
    "from scipy.special import huber\n",
    "from utils_polyfit import make_fake_data, show_data_func, true_func, loss_MSE, generate_mu_z, redshift_distribution\n",
    "# The below is to make polyfit a bit more quiet\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', np.RankWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "The simplest way to improve our situation is to fit the function (the polynomial in our case) with one set of data and use another set of data to assess the quality of the fit. So let us import the convenience function for this from `sklearn` - `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create some data just as before - but I make more because I want to split it into a test and training set. The \"right\" way to split data into test and training data is something we come back to - for now I will use a split of 50-50 but in general this is _not_ the optimal way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_fake_data(50, true_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_test_split` function returns the data split into two. I tend to prefer to define a list of indices for the arrays and then split these because that gives me more flexibility afterwards. To do that I do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(x), dtype=int)\n",
    "i_train, i_test = train_test_split(indices, test_size=0.5)\n",
    "print(\"The training set has {0} elements\".format(len(i_train)))\n",
    "print(\"The test set has {0} elements\".format(len(i_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[i_train], y[i_train], label='Train')\n",
    "plt.scatter(x[i_test], y[i_test], label='Test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a position where we can carry out fits on the training sample and evaluate the MSE on the test sample. I will also calculate two other quantities, the BIC and AIC which I'll mention below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_orders = 23 \n",
    "orders = np.arange(n_orders)\n",
    "\n",
    "MSE_train = np.zeros(n_orders)\n",
    "MSE_test = np.zeros(n_orders)\n",
    "BIC = np.zeros(n_orders)\n",
    "AIC = np.zeros(n_orders)\n",
    "\n",
    "best_fit = []\n",
    "N = len(i_train)\n",
    "\n",
    "for i, order in enumerate(orders):\n",
    "    # Fit the training sample using polyfit\n",
    "    p = np.polyfit(x[i_train], y[i_train], order)\n",
    "    best_fit.append(p)\n",
    "    \n",
    "    # Evaluate the best fit on the training sample\n",
    "    mu_fit_train = np.polyval(p, x[i_train])\n",
    "    MSE_train[i] = loss_MSE(y[i_train], mu_fit_train)\n",
    "    \n",
    "    # Calculate the best fit on the test sample\n",
    "    mu_fit_test = np.polyval(p, x[i_test])\n",
    "    MSE_test[i] = loss_MSE(y[i_test], mu_fit_test)\n",
    "\n",
    "    # And finally calculate the best information criterion\n",
    "    BIC[i] = 2*N*MSE_train[i] + order*np.log10(N)\n",
    "    AIC[i] = 2*order + 2*N*MSE_train[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the result\n",
    "\n",
    "We can now try to see what this tells us. We do this by plotting the MSE for the training and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(orders, MSE_train, label='train')\n",
    "plt.plot(orders, MSE_test, label='test')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xlabel('Polynomial order')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that indeed, the training MSE drops steadily, leading us to conclude the best model according to the training sample is the one with the highest polynomial order. In contrast, the test sample shows a clear increase in the MSE at some point - it will depend on the random seed in your run what you conclude! \n",
    "\n",
    "### Try\n",
    "Increase the number of points in the sample  and do the same run. What do you get? Is that as expected?\n",
    "\n",
    "### Try\n",
    "Run this a few times with 50 samples, what do you conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIC and AIC\n",
    "\n",
    "If we have few datapoints, the train-test split is not always feasible. In these cases (and many others as well!) you might prefer to use a simpler method to assess models. \n",
    "\n",
    "This is offered by _information criteria_. The two most popular are the [Bayesian Information Criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion) (BIC) and the [Akaike Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) (AIC), which are defined as \n",
    "\n",
    "$$\n",
    "\\mathrm{BIC} = M \\log_{10} N - 2\\ln L_{\\mathrm{max}},\n",
    "$$\n",
    "where $L_{\\mathrm{max}}$ is the maximum likelihood of the model (the best fit). If we assume that the likelihood is Gaussian (not a bad first assumption), the likelihood would have form \n",
    "$$\n",
    "L = \\prod_{i} \\frac{1}{\\sqrt{2\\pi}\\sigma_i} e^{-(y_i-y_{\\mathrm{pred},i})^2/2\\sigma},\n",
    "$$\n",
    "where the product is over the data in the sample. If we assume that the uncertainties are the same on all data points, then the log of the likelihood takes the form\n",
    "$$\\ln L = \\mathrm{const} - \\sum_i \\left( y_i - -y_{\\mathrm{pred},i}\\right)^2 = \\mathrm{const} - N \\mathrm{MSE}.$$\n",
    "\n",
    "Thus using this we can write the BIC as \n",
    "\n",
    "$$\n",
    "\\mathrm{BIC} = 2 N \\mathrm{MSE} + M \\log_{10} N,\n",
    "$$\n",
    "where $M$ is the number of parameters and $N$ the number of datapoints.  \n",
    "\n",
    "The AIC is formally defined as \n",
    "$$\n",
    "\\mathrm{AIC} = 2 M - 2 \\ln L_{\\mathrm{max}}\n",
    "$$\n",
    "which similarly is transformed into\n",
    "$$\n",
    "\\mathrm{AIC} = 2 M + 2 N \\mathrm{MSE}\n",
    "$$.\n",
    "\n",
    "We then look for the minimum BIC and AIC. I calculated these above so let us try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(orders, BIC, label='BIC')\n",
    "plt.plot(orders, AIC, label='AIC')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your run you likely will find a minimum for the AIC and BIC - this then represents the best fit model. And conveniently, it only needed the test data for this. This is a reason why frequently people choose to use AIC or BIC as a quick way to choose the best model and they can definitely be useful for that. In general, however, evaluation on a test set is considered a more rigorous way to assess quality of fit.\n",
    "\n",
    "The way to interpret the AIC and BIC scores is as relative log likelihoods so that if you have a set of models with minimum AIC of $a_m$ and another model with AIC $a_x$, then the likelihood of the second model relative to the minimum AIC model is $\\propto e^{a_m-a_x}$. Thus if you have two models that differ very little in AIC you may not have good evidence that it is significantly better than the other one. \n",
    "\n",
    "One note of caution in both cases: finding the best fitting model does not mean that it fits well! In this particular case, we know that the true function is a sinusoidal and we are therefore fitting with the wrong family of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How should I split my data? \n",
    "\n",
    "There is no rigorous analysis available for the optimal way to split a dataset into test and training sample, but we can make some general considerations:\n",
    "\n",
    "1. In general increasing the training sample reduces the random uncertainties on the parameter estimates. This is in general desirable (at least if the model is a decent fit to the data!)\n",
    "2. The size of the test sample is less important as long as it is reasonably large.\n",
    "\n",
    "These considerations have typically led to the recommendation that we split the sample into 80% training data and 20% test data. This is reasonable for moderately large samples, but if the number of data points in your samples reaches $10^6$ and above, a 80-20 split would lead to unnecessarily large test samples, and for that reason the recommendation for very large samples is to go down to splits like 99%/1%. The details are unlikely to be important as long has you have a few thousand samples in the test set.\n",
    "\n",
    "## Training, test and validation\n",
    "\n",
    "The idea of having a test set is to test the quality of the fitted model, removing the dependence on the training data. However, since the test set is used in the choice of the model, it is not really independent of the model fitting. For the example seen here this is not terribly problematic, but in large models this can be an important issue.\n",
    "\n",
    "For that reason, if you have enough data you should really split your data in three: \n",
    "\n",
    "- Training data. This is typically the largest chunk of the data and is used to fit the parameters of the model considered at each step. \n",
    "- Validation data. This is the data that are used to check the performance of the fitted model at each step (what we called test data above).\n",
    "- Test data. These are data that are not involved at all in the model fitting - they are kept outside. They are sometimes called the holdout data. We use these data at the end of the whole training process to give a final quantification of the quality of the fitted model.\n",
    "\n",
    "Unfortunately the literature, and the information you come across on the web, is not uniform in the choice of names for the last two sets and indeed you will sometimes see test and validation swapped. \n",
    "\n",
    "\n",
    "The `train_test_split` function does not have a function to split the data in three, but it is easy to do it in a two step process: first split into training and test+validation data and then split the test+validation data into test and validation data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbig, ybig = make_fake_data(500, true_func)\n",
    "indices = np.arange(len(xbig), dtype=int)\n",
    "i_train, i_test_validation = train_test_split(indices, train_size=0.8)\n",
    "i_test, i_validation = train_test_split(indices[i_test_validation], test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xbig[i_train], ybig[i_train], label='train', alpha=0.6)\n",
    "plt.scatter(xbig[i_validation], ybig[i_validation], label='validation')\n",
    "plt.scatter(xbig[i_test], ybig[i_test], label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "The test-training split is a bit ad hoc and while it often works quite well, it is not ideal for small and moderately sized data because you do not make use of all your data and you depend on the selection that is done, that it has not created some subtle systematic differences between your test and training data.\n",
    "\n",
    "For that reason it is often better to opt for cross-validation. Cross validation is like training-test splits but iterated over the whole data. The two figures below shows how this works for when you divide the sample into five groups (folds), what we call 5-fold cross-validation. \n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "\n",
    "<div>\n",
    "<img src=\"https://miro.medium.com/max/2736/1*rgba1BIOUys7wQcXcL4U5A.png\" width=\"480\"/>\n",
    "</div>\n",
    "\n",
    "`sklearn` has routines to do cross validation, so let us see how to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KFold` creates a set of train/test samples that we can loop over and do our inference on. `cross_validate` can be used to simplify the whole process. So to replicate the procedure above for the polynomial fitting we might do something like\n",
    "\n",
    "```\n",
    "kf = KFold(5)\n",
    "<.. Setup variables ..>\n",
    "\n",
    "for order in orders:\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        <... Setup the model - it should be cleared for each CV run ...>\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        <... Calculate MSE and CV score on test sample...>\n",
    "```\n",
    "\n",
    "Clearly the inner loop should then go in a function. And for reasons that will be clear soon, it is convenient to have this in a class that is `sklearn` compatible. So this can take the form like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Significantly extended from https://stackoverflow.com/questions/32660231/how-to-fit-a-polynomial-curve-to-data-using-scikit-learn\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class PolyEstimator(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, degree=2):\n",
    "        self.degree = degree\n",
    "        self.z = None\n",
    "        self.coefficients = None\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        params, resid, rank, singular_values, rcond = np.polyfit(x.flatten().tolist(), y, self.degree, full=True)\n",
    "        self.z = np.poly1d(params)\n",
    "        self.coefficients = self.z.c\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return loss_MSE(y, y_pred)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.z(x.flatten().tolist())\n",
    "\n",
    "    def __str__(self):\n",
    "        print(self.z)\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which can be used as follows:\n",
    "poly = PolyEstimator(degree=5)\n",
    "X = x[:, None]\n",
    "poly.fit(X, y)\n",
    "print(poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use a CV method to assess this kind of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_fit_model(X, y, degree, n_folds=5):\n",
    "    kf = KFold(n_folds)\n",
    "\n",
    "    # CV scores in our model is the mean square error\n",
    "    cv_scores = []\n",
    "\n",
    "    N = len(y)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        model = PolyEstimator(degree=degree)\n",
    "        model.fit(X_train, y_train)\n",
    "        cv_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    std_cv_score = np.std(cv_scores)\n",
    "\n",
    "    return mean_cv_score, std_cv_score\n",
    "\n",
    "# Checking that it works:\n",
    "# mean_cv, std_cv = cv_fit_model(X, y, 5, n_folds=5)\n",
    "# print(\"CV score = {0:.3f} +/- {1:.3f}\".format(mean_cv, std_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_folds = 10\n",
    "mean_cv_scores = np.zeros(len(orders))\n",
    "std_cv_scores = np.zeros(len(orders))\n",
    "for i, order in enumerate(orders):\n",
    "    mean_cv, std_cv = cv_fit_model(X, y, order, n_folds=N_folds)\n",
    "\n",
    "    mean_cv_scores[i] = mean_cv\n",
    "    std_cv_scores[i] = std_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(orders, mean_cv_scores, std_cv_scores, marker='o', fmt='none', color='gray')\n",
    "plt.scatter(orders, mean_cv_scores, marker='o', color='orange', zorder=10)\n",
    "plt.xlabel('Polynomial order')\n",
    "plt.ylabel('mean CV score [MSE]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue that you immediately can notice is that calculating standard deviation makes little sense when the scatter is large! Reduce the number of K-folds to see less scatter, or modify the code to use the 16th and 84th percentile instead to get appropriate asymmetric error bars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also simplify the process above because that particular way to run things is standard. This is what the `cross_validate` convenience function does for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cvs = np.zeros(len(orders))\n",
    "std_cvs = np.zeros(len(orders))\n",
    "for i, order in enumerate(orders):\n",
    "    cv = cross_validate(estimator=PolyEstimator(degree=order), \n",
    "        X=X, y=y, cv=5)\n",
    "    mean_cvs[i] = np.mean(cv['test_score'])\n",
    "    std_cvs[i] = np.std(cv['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we had already written a function to do the Cross-validation calculation, there was nothing really to be gained here but in general this saves us from writing that function and also it has a lot of other powerful aspects to it. \n",
    "\n",
    "Finally, we can also avoid writing the loop above by using `GridSearchCV`. This requires an estimator, a grid of parameters to explore and a cross-validation object. The grid of parameters is created using a dictionary with the keys equal to the keyword used to specify the parameters in the estimator ('degree' in our case here). So all the code above can be simplified into: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'degree': np.arange(1, 25)}\n",
    "cv = KFold(5)\n",
    "search = GridSearchCV(estimator=PolyEstimator(),         # Estimator to use\n",
    "                      param_grid=param_grid,             # The grid of parameters to explore\n",
    "                      scoring=\"neg_mean_squared_error\",  # The scoring function (many possibilities here)\n",
    "                      cv=cv)                             # The cross-validation object to use for the search\n",
    "search.fit(X, y)\n",
    "print(\"The best order = {0}\".format(search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You hopefully see that this is much more compact and _if_ you understand what you are doing, it is also much more readable. However, there is a risk that you get overly confident in the result - because if you look at how the MSE varies with order you are probably less confident that a degree of 5 is the best fit without doubt than if you used the `GridSearchCV` approach above.\n",
    "\n",
    "All that said, the `GridSearchCV` function is very useful and in complex situations can help streamline complex optimization problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:57:48.155068Z",
     "start_time": "2019-03-11T10:57:45.964087Z"
    }
   },
   "source": [
    "# Task: A polynomial fit to cosmological data\n",
    "\n",
    "The next three boxes creates a sample dataset of supernova Ia distances that you should fit with a polynomial. Find the best-fitting polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.cosmology import WMAP9 as cosmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:58:24.249945Z",
     "start_time": "2019-03-11T10:58:24.237661Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate data - see http://www.astroml.org/book_figures/chapter8/fig_gp_mu_z.html\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0, cosmo=cosmo)\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu_true = np.asarray(cosmo.distmod(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T11:35:06.542720Z",
     "start_time": "2019-03-11T11:35:06.367992Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(z_sample, mu_sample)\n",
    "plt.xlabel('Redshift')\n",
    "plt.ylabel('Distance modulus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
