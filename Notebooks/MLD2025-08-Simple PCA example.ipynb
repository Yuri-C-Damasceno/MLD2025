{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b03a48-97c5-42f7-a51b-11043d78bc5b",
   "metadata": {},
   "source": [
    "<table width=\"100%\">\n",
    "    <td align=\"left\">\n",
    "        <a target=\"_blank\", href=\"https://www.up.pt/fcup/en/\">\n",
    "            <img src=\"https://divulgacao.iastro.pt/wp-content/uploads/2023/03/FCUP_logo-print_blcktransp_600ppi.png\" width=\"90px\" height=\"90px\" style=\"padding-bottom:5px;\"/>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td>\n",
    "        <a target=\"_blank\", href=\"https://www.iastro.pt/\">\n",
    "            <img src=\"https://divulgacao.iastro.pt/wp-content/uploads/2018/03/IA_logo_bitmap-rgbblack-1200px-388x259.png\" width=\"90px\" height=\"90px\" style=\"padding-bottom:5px;\"/>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/jbrinchmann/MLD2025/blob/main/Notebooks/MLD2025-08-Simple%20PCA%20example.ipynb\">\n",
    "           <img src=\"https://tinyurl.com/3mm2cyk6\"  width=\"90px\" height=\"90px\" style=\"padding-bottom:5px;\"/>Run in Google Colab\n",
    "        </a>\n",
    "    </td>\n",
    "<td align=\"center\"><a target=\"_blank\" href=\"https://github.com/jbrinchmann/MLD2025/blob/main/Notebooks/MLD2025-08-Simple%20PCA%20example.ipynb\">\n",
    "<img src=\"https://tinyurl.com/25h5fw53\"  width=\"90px\" height=\"60px\" style=\"padding-bottom:0px;\"  />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a18a2",
   "metadata": {},
   "source": [
    "# Simple PCA example\n",
    "\n",
    "This simple PCA example goes with the lectures - it shows PCA on 1D example.  The original was done in IDL but I seem to have lost the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac295498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_plot(x, y, **kwargs):\n",
    "    \"\"\"Basic plot that we extend below\"\"\"\n",
    "    \n",
    "    plt.scatter(x, y, **kwargs)\n",
    "    plt.xlim(-30, 30)\n",
    "    plt.ylim(-30, 30)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel('Y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718aba78",
   "metadata": {},
   "source": [
    "## Create mock dataset\n",
    "\n",
    "The intrinsic function is\n",
    "$$\n",
    "y = 2 x\n",
    "$$\n",
    "but I add noise to both $x$ and $y$. This noise is normally distributed with standard deviation 1 for both $x$ and $y$. Since I add the noise to $x$ before $y$, the actual relation is\n",
    "$$\n",
    "y = 2 (x+\\epsilon)\n",
    "$$\n",
    "with $\\epsilon \\sim N(0, 1)$\n",
    "\n",
    "Note that I set the seed to a fixed value so that I can reproduce the values.\n",
    "\n",
    "There are many ways to modify this of course - one that you might find interesting is setting $y=2 x $ and then add noise to $y$ and $x$ afterwards and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77361b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = 100\n",
    "xmin = -3\n",
    "xmax = 3\n",
    "rng = np.random.default_rng(seed=1245)\n",
    "xtrue = rng.random(N_samples)*(xmax-xmin)+xmin\n",
    "x = xtrue + rng.normal(size=N_samples)*2+xtrue\n",
    "ytrue = 2*x\n",
    "y = rng.normal(size=N_samples)*3+ytrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a337c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_plot(x, y)\n",
    "plt.savefig(\"PCA-fig1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04922e73",
   "metadata": {},
   "source": [
    "## Calculating the covariance matrix of the data\n",
    "\n",
    "This is handled by `numpy`'s `cov` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.cov(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bca370",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.DataFrame(C)\n",
    "A.columns = ['']*A.shape[1]\n",
    "print(A.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1fc33",
   "metadata": {},
   "source": [
    "## Calculating the eigenvalues and eigenvectors\n",
    "\n",
    "This can be done in a variety of ways, some optimised for specific situations. We have at least the following options:\n",
    " - `np.linalg.eig` calculates eigenvalues and right eigenvectors for non-symmetric arrays\n",
    " - `np.linalg.eigh` does the same for real symmetric or complex Hermitian arrays\n",
    " - `np.linalg.eigvalsh` works like `np.eigh` but only calculates eigenvalues\n",
    " - `scipy.linalg.eigh` - very similar to `np.eigh` but has support for the [generalised eigenvalue problem](https://en.wikipedia.org/wiki/Generalized_eigenvector).\n",
    " \n",
    " Since covariance matrices are symmetric and in our case real, `eigh` is the most suitable option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_x = np.std(x)\n",
    "sig_y = np.std(y)\n",
    "mn_x = np.mean(x)\n",
    "mn_y = np.mean(y)\n",
    "C = np.cov((x-mn_x), (y-mn_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_vals, e_vecs = np.linalg.eigh(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Eigenvalues = \", e_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34cc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" First eigenvector\", e_vecs[:, 0])\n",
    "print(\"Second eigenvector\", e_vecs[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2971af",
   "metadata": {},
   "source": [
    "And show these eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54023354",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_plot(x, y, alpha=0.3)\n",
    "\n",
    "labels = [r\"$\\vec{u}_0$\", r\"$\\vec{u}_1$\"]\n",
    "dx = [0, +3]\n",
    "dy = [1.5, 0]\n",
    "for i in range(2):   \n",
    "    start = np.array([mn_x, mn_y])\n",
    "    end = start + 10*e_vecs[:, i]\n",
    "    \n",
    "    xl = [start[0], end[0]]\n",
    "    yl = [start[1], end[1]]    \n",
    "    plt.plot(xl, yl, color='red')\n",
    "    plt.annotate(labels[i], xy=(xl[1], yl[1]), xytext=(xl[1]+dx[i], yl[1]+dy[i]))\n",
    "#                arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "    \n",
    "plt.savefig(\"PCA-fig2.png\")\n",
    "\n",
    "ax=plt.gca()\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151822a",
   "metadata": {},
   "source": [
    "## An alternative derivation using SVD\n",
    "\n",
    "It is also possible to use singular value decomposition to get the eigenvectors. This is done using `np.linalg.svd` as show below. This particular approach avoids the creation of the covariance matrix as an intermediate step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0878c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(x)\n",
    "xData = np.reshape(x, (N, 1))\n",
    "yData = np.reshape(y, (N, 1))\n",
    "data = np.hstack((xData, yData))\n",
    "\n",
    "mu = data.mean(axis=0)\n",
    "data = data - mu\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(data.T, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_data = np.dot(data, eigenvectors)\n",
    "sigma = projected_data.std(axis=0).mean()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(12, 5))\n",
    "ax[0].scatter(xData, yData, alpha=0.5)\n",
    "ax[0].set_xlim(-30, 30)\n",
    "ax[0].set_ylim(-30, 30)\n",
    "for axis in eigenvectors:\n",
    "    start, end = mu, mu + sigma * axis\n",
    "    ax[0].annotate(\n",
    "        '', xy=end, xycoords='data',\n",
    "        xytext=start, textcoords='data',\n",
    "        arrowprops=dict(facecolor='red', width=2.0))\n",
    "#ax[0].set_aspect('equal')\n",
    "ax[0].set_xlabel(\"X\")\n",
    "ax[0].set_ylabel(\"Y\")\n",
    "\n",
    "ax[1].scatter(projected_data[:, 0], projected_data[:, 1])\n",
    "ax[1].set_xlabel(\"PC1\")\n",
    "ax[1].set_ylabel(\"PC2\")\n",
    "ax[1].set_ylim(-20, 20)\n",
    "\n",
    "plt.savefig(\"PCA-fig3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2eb7f",
   "metadata": {},
   "source": [
    "One can also clearly note the fact that the *direction* of the eigenvectors is defined only up to a sign. You can always reverse the eigenvectors without any implications for the further analysis.\n",
    "\n",
    "### Different languages - different rules\n",
    "It is also worth noting that different routines/languages return the eigenvalues/eigenvectors in different orders. In the `numpy` and `scipy` implementations we see here, they are sorted so that the smallest eigenvalue is first. This is also the case with Julia's `eigvals`/`eigvecs` functions, but R's `eigen` function returns the largest first.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008fb22",
   "metadata": {},
   "source": [
    "# Projection\n",
    "\n",
    "While the rotation of coordinate system seen above can be quite useful, PCA is usually done to reduce dimensionality. Since we have a 2D system, this is not terribly useful but the process is the same. To do that, we create a projection matrix, $M$, consisting of the eigenvectors we want to keep and project using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = e_vecs[:, 1]\n",
    "X = data # See above how this was created\n",
    "u = np.matmul(X, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c232867",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(u)), u)\n",
    "plt.ylabel(\"Projected value [eigenvector #2]\")\n",
    "plt.xlabel(\"Data number\")\n",
    "plt.savefig(\"PCA-fig4.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3eff14",
   "metadata": {},
   "source": [
    "But it is probably more useful to see this in the original space. To do that we need to multiply by the eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7958e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_recon = u*e_vecs[0, 1]\n",
    "y_recon = u*e_vecs[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_recon, y_recon, label=\"Reconstructed\", color='orange')\n",
    "plt.legend()\n",
    "plt.savefig(\"PCA-fig5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.3, label=\"Original\")\n",
    "plt.scatter(x_recon, y_recon, label=\"Reconstructed\", color='orange')\n",
    "plt.legend()\n",
    "plt.savefig(\"PCA-fig6.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3002835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally show orthogonal projection\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.3, label=\"Original\")\n",
    "plt.scatter(x_recon, y_recon, label=\"Reconstructed\", color='orange')\n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.plot([data[i, 0], x_recon[i]], [data[i, 1], y_recon[i]], ls='dotted')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(-20, 20)\n",
    "ax=plt.gca()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.savefig(\"PCA-fig7.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a3729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02d0dbe4",
   "metadata": {},
   "source": [
    "# How we actually do PCA\n",
    "\n",
    "In general we do not do all these steps manually - we usually use ready-made packages for this. In `sklearn` the natural choice is `sklearn.decomposition.PCA`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us do PCA with one component\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aebde9",
   "metadata": {},
   "source": [
    "The `fit` function runs PCA, while the `fit_transform` function runs PCA and projects the data to the new space. After projected data have been calculated, we can then use the `inverse_transform` function to go back to the original space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected = pca.fit_transform(X)\n",
    "X_recon = pca.inverse_transform(X_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579335be",
   "metadata": {},
   "source": [
    "And we can show that this gives the same projected data as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(X_recon[:, 0], X_recon[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0223a2c6",
   "metadata": {},
   "source": [
    "The advantage of the `sklearn` approach is of course that is easier, extends naturally to higher dimensions and can be chained into analysis pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccfa711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
