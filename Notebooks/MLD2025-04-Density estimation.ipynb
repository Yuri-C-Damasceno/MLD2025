{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=\"100%\">\n",
    "    <td align=\"left\">\n",
    "        <a target=\"_blank\", href=\"https://www.up.pt/fcup/en/\">\n",
    "            <img src=\"https://divulgacao.iastro.pt/wp-content/uploads/2023/03/FCUP_logo-print_blcktransp_600ppi.png\" width=\"90px\" height=\"90px\" style=\"padding-bottom:5px;\"/>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td>\n",
    "        <a target=\"_blank\", href=\"https://www.iastro.pt/\">\n",
    "            <img src=\"https://divulgacao.iastro.pt/wp-content/uploads/2018/03/IA_logo_bitmap-rgbblack-1200px-388x259.png\" width=\"90px\" height=\"90px\" style=\"padding-bottom:5px;\"/>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/jbrinchmann/MLD2025/blob/main/Notebooks/MLD2025-01b-Deciding%20when%20good%20is%20good%20enough.ipynb\">\n",
    "           <img src=\"https://tinyurl.com/3mm2cyk6\"  width=\"90px\" height=\"90px\" style=\"padding-bottom:5px;\"/>Run in Google Colab\n",
    "        </a>\n",
    "    </td>\n",
    "<td align=\"center\"><a target=\"_blank\" href=\"https://github.com/jbrinchmann/MLD2025/blob/main/Notebooks/MLD2025-01b-Deciding%20when%20good%20is%20good%20enough.ipynb\">\n",
    "<img src=\"https://tinyurl.com/25h5fw53\"  width=\"90px\" height=\"60px\" style=\"padding-bottom:0px;\"  />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density estimation\n",
    "\n",
    "Density estimation concerns trying to infer an underlying density/probability distribution from a set of observations. The methods used in standard machine learning typically do not account for uncertainties in the measurements which is sub-optimal for astronomy. \n",
    "\n",
    "Here we will explore a couple of different approaches and also see how to estimate the parameters of the different methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "# !wget --quiet -O catalog-of-pulsars.vot https://www.dropbox.com/scl/fi/dv2tnpo1lqoo20xzv1in8/catalog-of-pulsars.vot?rlkey=e9d6biwnr871xfrcl928pvmha&dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from astropy.table import Table\n",
    "from astropy.visualization import hist\n",
    "\n",
    "# Note that astropy also has a modification of hist that does the same as this one.\n",
    "#from astroML.plotting import hist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A table of pulsars\n",
    "\n",
    "This was downloaded from [Vizier](http://vizier.u-strasbg.fr/cgi-bin/VizieR) - table 1 from [Taylor et al (1995)'s Catalog of pulsars](http://vizier.u-strasbg.fr/viz-bin/VizieR?-source=VII/189). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tPulsar = Table().read('../Datafiles/catalog-of-pulsars.vot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Galactic latitude distribution\n",
    "\n",
    "\n",
    "Pulsars tend to be in the Galactic plane. To check whether this sample shows this we can create a histogram of the data. The following does this in the simplest possible way - using the default histogram function `hist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "h = hist(tPulsar['GLAT'], ax=ax)\n",
    "ax.set_xlabel('Galactic Latitude')\n",
    "ax.set_ylabel('# pulsars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not terribly nice - the bins are too big. If we want to stick with histograms, there are two main ways forwards: \n",
    "\n",
    "- using fixed size bins but using a more sophisticated method to estimate the bin width.\n",
    "- using a flexible bin size that adapts to the data.\n",
    "\n",
    "These have different advantages and disadvantages which we can discuss but to explore them it is useful to have a little convenience function to show one histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data, binning_style, label, ax=None, xmin=-90, xmax=90, quiet=False):\n",
    "    \"\"\"\n",
    "    Convenience routine for plotting a histogram.\n",
    "    \"\"\"\n",
    "\n",
    "    if ax is not None:\n",
    "        ax = plt.axes(ax)\n",
    "\n",
    "    # Handle NaN's gracefully.\n",
    "    keep = np.where(~np.isnan(data))\n",
    "    \n",
    "    counts, bins, patches = hist(data[keep], bins=binning_style, ax=ax,\n",
    "                                 color='k', histtype='step', density=True)\n",
    "\n",
    "    if not quiet:\n",
    "        ax.text(0.95, 0.93, '{0:s}:\\n{1:d} bins'.format(label, len(counts)),\n",
    "                transform=ax.transAxes, ha='right', va='top')\n",
    "\n",
    "    # Turn off y-axis labeling.\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this function to show four different methods to estimate the density. The first three use fixed bin sizes but have different ways to estimate the bin size. The last, the Bayesian blocks method, has flexible bin sizes but unfortunately the implementation seems to show a bug. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "styles = ['scott', 'freedman', 'knuth', 'blocks']\n",
    "titles = [\"Scott's\", 'Free.-Diac.', 'Knuth', 'Bayesian blocks']\n",
    "for i in range(len(styles)):\n",
    "\n",
    "    axis = plot_histogram(tPulsar['GLAT'], styles[i], titles[i], ax=ax[i])\n",
    "    ax[i].set_xlabel('Galactic latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to modify these histogramming routines, and for certain problem cases this is quite desirable - you can for instance ask for a minimum and maximum number of data points per bin, or create histograms with bins that are shifted and averaged to get a smoother histogram. See for instance [this Github project](https://github.com/ajdittmann/ash) for an implementation of these average smoothed histograms (ASH), or you can roll your own.\n",
    "\n",
    "Another situation where you might want to have a modified histogram routine is if you want to take into account the uncertainties on your observations. One way to do this is to put the histogram call in a bootstrap and Monte Carlo loop and calculate a histogram with an uncertainty estimate on each bin.\n",
    "\n",
    "But even the standard histogram can be very useful in certain situations. Say for instance that you want to calculate the average rotation measure in each bin in galactic latitude. You can do this by first calculating the histogram bins for the latitute and then use those bins to calculate the average [dispersion measure](https://astronomy.swin.edu.au/cosmos/*/Pulsar+Dispersion+Measure) (DM in the table) using code like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the bins for the latitude data.\n",
    "data = tPulsar['GLAT']\n",
    "keep = np.where((~np.isnan(data)) & (np.abs(data) < 100))\n",
    "counts, bin_edges = np.histogram(data[keep], bins='doane')\n",
    "bins = [[bin_edges[i], bin_edges[i+1]] for i in range(len(bin_edges)-1)]\n",
    "\n",
    "#\n",
    "# Then use np.digitize to calculate the bin each data point belongs to\n",
    "# And then loop over the bins and calculate the average of all points\n",
    "# that end up in that bin.\n",
    "#\n",
    "inds = np.digitize(data, bin_edges)\n",
    "mean_DM = np.zeros(len(bins))\n",
    "sd_DM = np.zeros(len(bins))\n",
    "xDM = tPulsar['DM']\n",
    "xdata = np.zeros(len(bins))\n",
    "for i, b in enumerate(bins):\n",
    "    in_this_bin, = np.where((inds == i) & (~np.isnan(xDM)) & (~np.isnan(data)))\n",
    " \n",
    "    if (len(in_this_bin) > 1):\n",
    "        mean_DM[i] = np.mean(xDM[in_this_bin])\n",
    "        sd_DM[i] = np.std(xDM[in_this_bin])\n",
    "        xdata[i] = np.mean(data[in_this_bin])\n",
    "    else:\n",
    "        mean_DM[i] = np.nan\n",
    "        sd_DM[i] = np.nan\n",
    "        xdata[i] = 0.5*(b[0]+b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xdata, mean_DM, marker='o', ls='dashed')\n",
    "plt.errorbar(xdata, mean_DM, sd_DM, marker='o')\n",
    "#plt.ylim(-10, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might find it interesting(?) to figure out why the distribution and its scatter has the shape it has. For that you might find it interesting to look at the distance - `dist` in the table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel density estimates\n",
    "\n",
    "With a kernel density estimate we can get a smoother estimate of the distribution and this is often useful, in particular if you need a derivative of the distribution! To do a kernel density estimate, we can use the `sklearn.neighbors` `KernelDensity` package. Before we do this, it might be useful to illustrate the different kernels available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the kernels over -5 to 5\n",
    "from matplotlib.lines import Line2D\n",
    "x = np.linspace(-5, 5, 5000)\n",
    "dx = x[1] - x[0]\n",
    "\n",
    "kernels = {'gauss': (1. / np.sqrt(2 * np.pi)) * np.exp(-0.5 * x ** 2),\n",
    "           'exponential': 0.5 * np.exp(-abs(x)),\n",
    "           'tophat': 0.5*np.ones_like(x)*(abs(x) < 1),\n",
    "           'epanechnikov': 0.75*(1-x*x)*(abs(x) < 1)}\n",
    "           \n",
    "\n",
    "fig, axes = plt.subplots(1, 4, sharey=True, frameon=False)\n",
    "fig.set_size_inches(15, 5)\n",
    "fig.set_frameon(False)\n",
    "# fig.figsize(5,15)\n",
    "for (k, ax) in zip(kernels, axes):\n",
    "\n",
    "    ax.plot(x, kernels[k], '-', c='black')\n",
    "    ax.fill(x, kernels[k], '-k', fc='#AAAAFF')\n",
    "\n",
    "    # This turns off the tick marks on the y-axis and removes the bars on the left, \n",
    "    # right and top. \n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "\n",
    "    ax.set_xlim(-5, 5)    \n",
    "    \n",
    "    ax.set_title(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key thing you can see in this figure is that the two kernels on the left both are limited to a finite range in x (they have finite support), while the exponential and Gaussian kernels extend to infinity (have infinite support). This can have implementation consequences - in some cases a kernel with finite support can be used to speed up calculations. The Epanechnikov kernel in addition has the nice theoretical property that it minimises variance (is least sensitive to a swap of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use the KernelDensity class from Scikit-learn.\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# from astroML.datasets import fetch_great_wall\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way the kernel density estimators in `scikit-learn` work is pretty much the same way as any machine learning method in `sklearn`, thus they can be coupled into a pipeline if that is convenient for your work.\n",
    "\n",
    "First you fit the model to your observable (`.fit` below), then you evaluate the fitted model on new data. This is done with the `score_samples` routine below.\n",
    "\n",
    "So let us try this out for the problem above to see how it works.\n",
    "\n",
    "We first extract the data we want to fit (for convenience). Note that sklearn in general wants a 2D array as input - it will, for the moment, work for 1D arrays but complain. The `[:, None]` construction below adds a new final dimension to the otherwise 1D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tPulsar['GLAT'][:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to create the model we want to fit the data with. This is the `KernelDensity` model which requires two parameters: the bandwidth of the kernel - we will see below how to rigorously estimate this for now I use an eyeball value, and the type of kernel. I use the Gaussian kernel which in general works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KDE_model = KernelDensity(bandwidth=5, kernel='gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we fit the data with this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_GLAT = KDE_model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this call is an updated KernelDensity object that is now fit. Thus we can now input new values into the object and get predicted values out. To do this we use the `score_samples` function - this takes the x-values as input and gives out the log likelihood for that x value given the fitted KDE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create  the grid on which to evaluate the results\n",
    "x_GLAT = np.linspace(-90, 90, 512)\n",
    "x_GLAT = x_GLAT[:, None] # Oh yes, it needs to be 2D!\n",
    "\n",
    "# Then evaluate the KDE solutions\n",
    "log_dens_GLAT = kde_GLAT.score_samples(x_GLAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that to have a plot that can be compared to the previous histograms we need to exponentiate!\n",
    "plt.plot(x_GLAT, np.exp(log_dens_GLAT))\n",
    "plt.xlabel('Galactic longitude')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A KDE estimate for parallax\n",
    "\n",
    "Here we will take the ideas above and apply them to the parallax of a subset of stars from Gaia. The column you need is `parallax`. \n",
    "\n",
    "What you need to do is:\n",
    "\n",
    "1. Create a density estimate of the parallax using the three different kernels: `gaussian`, `tophat`, `exponential`, and `epanechnikov` and compare the result.\n",
    "2. Determine the best bandwidth for one of these kernels and show how well constrained this bandwidth is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are on colab or haven't got the gaia stars file - uncomment the line below and download the file\n",
    "# !wget --quiet -O gaia-stars.vot https://github.com/jbrinchmann/MLD2025/raw/refs/heads/main/Datafiles/gaia-stars.vot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Table().read(\"gaia-stars.vot\", format='votable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t['parallax'][:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the fit\n",
    "kde = KernelDensity(bandwidth=0.1, kernel='gaussian').fit(X)\n",
    "kde_th = KernelDensity(bandwidth=0.1, kernel='tophat').fit(X)\n",
    "kde_exp = KernelDensity(bandwidth=0.1, kernel='exponential').fit(X)\n",
    "kde_ep = KernelDensity(bandwidth=0.1, kernel='epanechnikov').fit(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create  the grid on which to evaluate the results\n",
    "xout = np.linspace(0, 1.5, 512)\n",
    "xout = xout[:, None]\n",
    "\n",
    "# Then evaluate the KDE solutions\n",
    "log_dens = kde.score_samples(xout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xout, log_dens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot I will first plot the results using the Gaussian kernel and overplot a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(xout, np.exp(log_dens))\n",
    "fig, ax = plt.subplots()\n",
    "y = t['parallax']\n",
    "keep = np.where(~np.isnan(y))\n",
    "ax = plot_histogram(y[keep], 'knuth', 'parallax', ax=plt.gca(), xmin=0, xmax=1.5)\n",
    "ax.set_xlabel('Gaia parallax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us now compare the different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'gauss': kde,\n",
    "           'exponential': kde_exp,\n",
    "           'tophat': kde_th,\n",
    "           'epanechnikov': kde_ep}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(12, 2.5))\n",
    "for i, key in enumerate(results.keys()):\n",
    "    log_density = results[key].score_samples(xout)\n",
    "    axes[i].plot(xout, np.exp(log_density))\n",
    "    axes[i].set_title(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exponential = kde_exp # \n",
    "log_likelihood = model_exponential.score_samples(xout)\n",
    "plt.plot(xout, np.exp(log_likelihood))\n",
    "\n",
    "model_gaussian = kde # \n",
    "log_likelihood = model_gaussian.score_samples(xout)\n",
    "plt.plot(xout, np.exp(log_likelihood))\n",
    "\n",
    "model_tophat = kde_th # \n",
    "log_likelihood = model_tophat.score_samples(xout)\n",
    "plt.plot(xout, np.exp(log_likelihood), color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part below calculates the corss-validation score for a series of bandwidths. This is not the most elegant way to solve this - I'll show an alternative using `GridSearchCV` below, but this version is very explicit about what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cv1(x, bws, model='gaussian', plot=False, n_splits=10):\n",
    "    \"\"\"\n",
    "    Estimate bandwidth using leave-one-out cross-validation\n",
    "    \n",
    "    Input:\n",
    "       x: data (assumed 1D)\n",
    "       bws: Bandwidths to test (assumed 1D)\n",
    "       \n",
    "    Keywords:\n",
    "       model: The kernel to use (default='gaussian')\n",
    "       plot: Set this to true to show a big grid of the test and training\n",
    "             samples with the KDE chosen at each step. This is not robust!\n",
    "       n_splits: the number of folds to split the data into. Default=10.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the number of bandwidths to check and the number of objects\n",
    "    N_bw = len(bws)\n",
    "    N = len(x)\n",
    "    cv_1 = np.zeros(N_bw)\n",
    "    \n",
    "    # If plotting is requested, set up the plot region\n",
    "    if plot:\n",
    "        fig, axes = plt.subplots(N_bw, np.ceil(N/n_splits), figsize=(15, 8))\n",
    "        xplot = np.linspace(np.min(x), np.max(x), 1000)\n",
    "\n",
    "    # Loop over each band-width and calculate the probability of the \n",
    "    # test set for this band-width\n",
    "    for i, bw in enumerate(bws):\n",
    "    \n",
    "        # I will do N-fold CV here. This divides X into N_folds\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "\n",
    "        # Initiate - lnP will contain the log likelihood of the test sets\n",
    "        # and i_k is a counter for the folds that is used for plotting and\n",
    "        # nothing else..\n",
    "        lnP = 0.0\n",
    "        i_k = 0\n",
    "                                 \n",
    "        # Loop over each fold\n",
    "        for train, test in kf.split(X):\n",
    "            x_train = x[train, :]\n",
    "            x_test = x[test, :]\n",
    "            \n",
    "            # Create the kernel density model for this bandwidth and fit\n",
    "            # to the training set.\n",
    "            kde = KernelDensity(kernel=model, bandwidth=bw).fit(x_train)\n",
    "                                 \n",
    "            # score evaluates the log likelihood of a dataset given the fitted KDE.\n",
    "            log_prob = kde.score(x_test)\n",
    "            \n",
    "            if plot:\n",
    "                # Show the tries\n",
    "                ax = axes[i][i_k]\n",
    "\n",
    "                # Note that the test sample is hard to see here.\n",
    "                hist(x_train, bins=10, ax=ax, color='red')\n",
    "                hist(x_test, bins=10, ax=ax, color='blue')\n",
    "                ax.plot(xplot, np.exp(kde.score_samples(xplot[:, np.newaxis])))\n",
    "                i_k += 1\n",
    "            \n",
    "\n",
    "            lnP += log_prob\n",
    "            \n",
    "        # Calculate the average likelihood          \n",
    "        cv_1[i] = lnP/N\n",
    "        \n",
    "    return cv_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with this function, we can now get the cross-validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t['parallax'][:, np.newaxis] # A different way to add a new axis = [:, None]\n",
    "bws = np.linspace(0.05, 1, 50)\n",
    "cv = cv1(X, bws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bws, np.exp(cv))\n",
    "plt.xlabel('bandwidth')\n",
    "plt.ylabel('CV likelihood')\n",
    "plt.text(4, 0.001, 'Best BW={0:.4f}'.format(bws[np.argmax(cv)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can improve (possibly) on this estimate by interpolation - this is more useful when you calculate the cross-validation score on a coarse grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_big = np.linspace(0.05, 1, 1000)\n",
    "cv_big = np.interp(bw_big, bws, cv)\n",
    "print(\"The linearly interpolated maximum is={0:.4f}\".format(bw_big[np.argmax(cv_big)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "f = interpolate.interp1d(bws, cv, kind=3)\n",
    "cv_big2 = f(bw_big)\n",
    "print(\"The quadratically interpolated maximum is={0:.4f}\".format(bw_big[np.argmax(cv_big2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U will use the quadratically interpolated maximum as the best\n",
    "best_bw = bw_big[np.argmax(cv_big2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice the two are very close but the maximum is not terribly well constrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bws, np.exp(cv), 'o')\n",
    "plt.plot(bw_big, np.exp(cv_big), 'r', label='linear interpolation')\n",
    "plt.plot(bw_big, np.exp(cv_big2), 'b', label='quadratic interpolation')\n",
    "plt.xlim(0, 1)\n",
    "#plt.ylim(0.011, 0.0132)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let us see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensity(bandwidth=best_bw, kernel='gaussian').fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the output grid defined above.\n",
    "\n",
    "# Then evaluate the KDE solutions\n",
    "log_dens = kde.score_samples(xout)\n",
    "\n",
    "plt.plot(xout, np.exp(log_dens))\n",
    "ax = plot_histogram(t['parallax'], 'knuth', 'Parallax distribution', ax=plt.gca(), xmin=0, xmax=1.5)\n",
    "ax.set_xlabel('Parallax')\n",
    "ax.set_title('Best bandwidth from CV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A version using GridSearchCV \n",
    "\n",
    "The code above is quite verbose, and if we wanted to also optimise over the kernel type we would need to extend it a bit. An alternative that takes less lines is to use GridSearchCV - this is shown below. To have a reasonable execution time, I would recommend using a k-fold of 5 rather than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'bandwidth': bws, 'kernel': ['gaussian']}\n",
    "cv = KFold(5)\n",
    "search = GridSearchCV(estimator=KernelDensity(),\n",
    "                      param_grid=param_grid, cv=cv)                           \n",
    "search.fit(X, y)\n",
    "print(\"The best order = {0}\".format(search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bws, search.cv_results_['split0_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian mixture models\n",
    "\n",
    "Another way to fit a distribution is to use Gaussian mixture models. These are basically combinations of Gaussians where we write that the distribution is defined as \n",
    "$$p(x_i|\\theta) = \\sum_{j=1}^M \\alpha_j N(x_i; \\mu_j, \\sigma_j^2)$$\n",
    "with $M$ being the number of Gaussians (a hyper-parameter) and where I have specialised to 1D so that the covariance matrix reduces to a single variance, $\\sigma_j^2$. We can use this both to infer the properties of an unknown (or known) set of Gaussians or just as a convenient way to describe a density distibution. \n",
    "\n",
    "In `sklearn` we do this as follows (for two Gaussians):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "model = GaussianMixture(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the means of the two Gaussians and their widths using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The means of the Gaussian components = \", res.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The covariances (here sigmas) = \", res.covariances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now look at this as the combination of two likelihoods, we can classify each data point as belonging to one Gaussian or the other using the `predict` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = res.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(t['parallax'][labels==0], t['b'][labels==0], color='red')\n",
    "plt.scatter(t['parallax'][labels==1], t['b'][labels==1], color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustrates a common issue with Gaussian expansions - the component with largest $\\sigma$ tends to dominate at large radii - whether this is physically meaningful or not is up to you to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualise the results, we use the `score_sample` function on data along the axis we want to sample. This is a 1D example - in the (maybe more common?) 2D case you would create a 2D `xout` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = model.score_samples(xout)\n",
    "plt.plot(xout, np.exp(log_likelihood), color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we need to be rigorous and estimate the optimal number of Gaussian components as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_components': np.arange(1, 20)}\n",
    "cv = KFold(10)\n",
    "search = GridSearchCV(estimator=GaussianMixture(),\n",
    "                      param_grid=param_grid, cv=cv)                           \n",
    "search.fit(X, y)\n",
    "print(\"The best number of components = {0}\".format(search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianMixture(n_components=search.best_params_['n_components'])\n",
    "res = model.fit(X)\n",
    "log_likelihood = model.score_samples(xout)\n",
    "\n",
    "plt.plot(xout, np.exp(log_dens), label='Kernel Density')\n",
    "plt.plot(xout, np.exp(log_likelihood), color='red', label='Gaussian Mixture')\n",
    "ax = plot_histogram(t['parallax'], 'knuth', 'Parallax distribution', ax=plt.gca(), xmin=0, xmax=1.5, quiet=True)\n",
    "ax.set_xlabel('Parallax')\n",
    "ax.set_title('Best bandwidth from CV')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
