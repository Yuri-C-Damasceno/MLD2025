{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bca66f24-906f-4a6b-ac26-049c2fda7627",
   "metadata": {},
   "source": [
    "<table>\n",
    "<td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/jbrinchmann/MLD2025/blob/main/Notebooks/MLD2025-01a-polynomial%20fitting.ipynb\">\n",
    "       <img src=\"https://tinyurl.com/3mm2cyk6\"  width=\"90px\" height=\"90px\" style=\"padding-bottom:5px;\"/>Run in Google Colab</a></td>\n",
    "<td align=\"center\"><a target=\"_blank\" href=\"https://github.com/jbrinchmann/MLD2025/blob/main/Notebooks/MLD2025-01a-polynomial%20fitting.ipynb\">\n",
    "<img src=\"https://tinyurl.com/25h5fw53\"  width=\"90px\" height=\"60px\" style=\"padding-bottom:0px;\"  />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb222a3-21aa-4c20-b0d1-1f04c244120a",
   "metadata": {},
   "source": [
    "# Running this notebook\n",
    "\n",
    "You have two options for running this notebook - you can clone the repository to your computer and run the notebook there. That is ideal if the internet is so-so and you will work on it a bit here and a bit there. Or you can click on the Google Colab link above and run the notebook in Google Colab. To do that, you need to have a Google account, obviously you need to be online, and you need to be aware that you will have to download datafiles each time you use it (you can set it up to [load from your Google Drive](https://colab.research.google.com/notebooks/io.ipynb) if that is preferrable), and there is a time-out. However, the big advantage is that you usually do not have to install any packages which is very convenient for trying out things in class. If you do need to install a package, `astroML` is an example, then you can enter:\n",
    "```\n",
    " !pip install astroML\n",
    "```\n",
    "in a cell (typically at the top of the notebook) and the package will be installed for you for that session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4715ad-a41a-4c69-8663-93192c622792",
   "metadata": {
    "id": "2f4715ad-a41a-4c69-8663-93192c622792"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# The below is to make polyfit a bit more quiet\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', np.RankWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d13ec2-fc85-4a10-93f1-8765e41283c1",
   "metadata": {
    "id": "d4d13ec2-fc85-4a10-93f1-8765e41283c1"
   },
   "source": [
    "# A first dive into machine learning\n",
    "\n",
    "$\\newcommand{\\pred}[1]{#1_{\\mathrm{pred}}}$\n",
    "\n",
    "# Accuracy and error-rate or loss function\n",
    "\n",
    "In any machine learning situation we need to be able to quantify how well our method is working. This is done by what is known as an _error function_ or _loss function_. There are many ways one might measure the quality of a fit and they all have different strengths/advantages.\n",
    "\n",
    "One particular family is the $L^n$ norms. If we denote the actual observed data as $y$ and the values predicted by the model as $\\pred{y}$ --- the most generic version of this has an error rate function of the form\n",
    "$$\\epsilon = ||y-\\pred{y}||_p = \\left( \\sum_i \\left|y-\\pred{y}\\right|^p\\right)^{1/p}$$\n",
    "\n",
    "For $p=2$ we have arguably the most widely used norm, the least squares norm. The square of this is often used for convenience, obviously if you are looking for the minimum this is does not matter, and commonly named the mean square error (MSE): \n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{N} \\sum_i^N \\left(y-\\pred{y}\\right)^2.$$\n",
    "\n",
    "This is in general a well-behaving norm when you do not have too many outliers in your data and has nice analytic properties. Other interesting norms are the $L^1$ norm and even then $L^0$ norm which have as advantage that they will typically give \\emph{sparse} solutions - where some parameters of the model are set to zero, but they are much less nice analytically so it is a trade-off.\n",
    "\n",
    "It should be remarked that the subject of loss functions is a lot broader than I make it sound here - we will look at a few more here but I will not aim for any kind of completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfe482-9a96-4160-8510-27d7afe1d997",
   "metadata": {
    "id": "JD_iQgZxGv-0"
   },
   "source": [
    "## Making fake data \n",
    "\n",
    "In order to see the use of an error function, we are first going to create some fake data. The function below takes a function as an argument. Here I will create a the data around a linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e1cb2-d8df-4381-9dd9-9aef410edee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fake_data(N, func, noise_scale=0.5):\n",
    "  \"\"\"Create fake data using a given function\n",
    "\n",
    "  Arguments\n",
    "  ---------\n",
    "      N: int\n",
    "         The number of data points to return\n",
    "      func: function\n",
    "         The function to call to get the data. This must take \n",
    "         one argument (the x-value) and return the y-value.\n",
    "\n",
    "  Keywords\n",
    "  --------\n",
    "      noise_scale : float, default 0.5\n",
    "         The variance of the normal distribution that is\n",
    "         used to assigned random noise to the data. Set to\n",
    "         zero to add no noise.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "      xvalues : numpy array\n",
    "         The x-values used for calculating the data points \n",
    "      yvalues : numpy array\n",
    "         The y-values, using true_func and then adding noise.\n",
    "  \"\"\"\n",
    "\n",
    "  rng = np.random.default_rng()\n",
    "  xvalues = rng.uniform(0, 4, N)\n",
    "  yvalues = func(xvalues)+rng.normal(0, noise_scale, len(xvalues))\n",
    "\n",
    "  return xvalues, yvalues\n",
    "\n",
    "\n",
    "def show_data_func(x, y, func, xrange=None, yrange=None, ax=None):\n",
    "  \"\"\"A convenience function for showing data points and the true function\n",
    "  \n",
    "  Arguments\n",
    "  ---------\n",
    "      x : numpy array\n",
    "          The x-values for the data\n",
    "      y : numpy array\n",
    "          The y-values for the data\n",
    "      func : function\n",
    "          The true function to overplot the data\n",
    "          \n",
    "  Keywords:\n",
    "  ---------\n",
    "      xrange : two-element array, optional\n",
    "          The x-range to show.\n",
    "      yrange : two-element array, optional\n",
    "          The y-range to show.\n",
    "      ax : matplotlib Axis object, optional\n",
    "          The axis object to plot into, if not \n",
    "          provided, an axis object is created and\n",
    "          returned.\n",
    "  \"\"\"\n",
    "  xplot = np.linspace(np.min(x), np.max(x), 500)\n",
    "  yplot = func(xplot)\n",
    "  if (ax is None):\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1)\n",
    "  ax.scatter(x, y, label='Data points')\n",
    "  ax.plot(xplot, yplot, label='True function', color='orange')\n",
    "  ax.set_xlabel('x')\n",
    "  ax.set_ylabel('y')\n",
    "  if xrange is not None:\n",
    "    ax.set_xlim(xrange)\n",
    "  if yrange is not None:\n",
    "    ax.set_ylim(yrange)\n",
    "\n",
    "  ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732f06a-3b9c-4570-a7c9-e1ee47d2c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_func(x):\n",
    "    return x\n",
    "    \n",
    "def get_linear_data():\n",
    "    xval, yval = make_fake_data(20, linear_func)\n",
    "    return xval, yval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b35b08-a5f0-4971-a645-bcf0769bede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl, yl = get_linear_data()\n",
    "show_data_func(xl, yl, linear_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348de8fa-d848-4acf-8985-76c649aa90fe",
   "metadata": {},
   "source": [
    "Now, let us define a few loss/error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9ff6e-5219-4094-86a5-bfe0a86ee499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Huber loss function\n",
    "from scipy.special import huber\n",
    "\n",
    "def loss_Lp(y, y_pred, p):\n",
    "    \"\"\"L^p loss function for p > 0\"\"\"\n",
    "    if p == 0:\n",
    "        print(\"This loss function is for p > 0 - use loss_L0 for p=0\")\n",
    "        return None\n",
    "\n",
    "    if p == 1:\n",
    "        z = np.sum(np.abs(y-y_pred))\n",
    "    else:\n",
    "        z = np.sum((y-y_pred)**p)\n",
    "        \n",
    "    return z**(1/p)\n",
    "\n",
    "def loss_L0(coeffs, tol=1e-15):\n",
    "    \"\"\"L0 loss function - this just counts the non-zero coefficients\"\"\"\n",
    "\n",
    "    is_zero, = np.where(np.abs(coeffs) < tol)\n",
    "    loss = len(is_zero)\n",
    "    return L0\n",
    "\n",
    "def loss_10(y, y_pred, tol=1e-15):\n",
    "    \"\"\"0-1 loss with tolerance\"\"\"\n",
    "    diff = np.abs(y-y_pred)\n",
    "    \n",
    "    return loss_L0(diff)\n",
    "\n",
    "def loss_MAE(y, y_pred):\n",
    "    \"\"\"Median absolute error\"\"\"\n",
    "    N = len(y)\n",
    "    return loss_Lp(y, y_pred, 1)/N\n",
    "\n",
    "def loss_MSE(y, y_pred):\n",
    "    \"\"\"Mean square error\"\"\"\n",
    "    N = len(y)\n",
    "    return np.sum((y-y_pred)**2)/N\n",
    "\n",
    "def loss_Huber(y, y_pred, delta=1.0):\n",
    "    \"\"\"Huber loss function\"\"\"\n",
    "    N = len(y)\n",
    "    return np.sum(huber(delta, y-y_pred))/N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656878f-865c-4673-bbbc-4a467c0fc277",
   "metadata": {},
   "source": [
    "## Find the best linear fit\n",
    "\n",
    "I am not going to find this in an efficient way - the method used below to fit a polynomial is a better choice but here we want to try different slopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeffaea-0a8f-47f7-af71-87661219627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fit_to_linear(x, y, slopes=np.arange(-0.5, 2.5, 0.01), loss_function=loss_MSE):\n",
    "    \"\"\"Loop over the possible slopes\"\"\"\n",
    "\n",
    "    loss = np.zeros(len(slopes))\n",
    "    for i, slope in enumerate(slopes):\n",
    "        y_pred = x*slope\n",
    "        loss[i] = loss_function(y, y_pred)\n",
    "\n",
    "    return slopes, loss\n",
    "\n",
    "def show_fit(s, loss, label, ax):\n",
    "    ax.plot(s, loss, label=label)\n",
    "    i_min = np.argmin(loss)\n",
    "    print(\"{0}: best slope={1:.3f}\".format(label, s[i_min]))\n",
    "    ax.plot(s[i_min], loss[i_min], marker='o', markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444330a-8751-456f-9e27-86fa38688ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, l_MSE = calculate_fit_to_linear(xl, yl, loss_function=loss_MSE)\n",
    "slopes, l_MAE = calculate_fit_to_linear(xl, yl, loss_function=loss_MAE)\n",
    "slopes, l_Huber = calculate_fit_to_linear(xl, yl, loss_function=loss_Huber)\n",
    "\n",
    "# When comparing I take the square root of the MSE to be with similar units\n",
    "fig, ax = plt.subplots(ncols=1, nrows=1)\n",
    "show_fit(slopes, np.sqrt(l_MSE), 'MSE', ax)\n",
    "show_fit(slopes, l_MAE, 'MAE', ax)\n",
    "show_fit(slopes, l_Huber, 'Huber', ax)\n",
    "ax.axvline(1.0, color='orange', ls='dashed')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ed6b1-d45a-469a-aecb-b47e53db7631",
   "metadata": {
    "id": "JD_iQgZxGv-0"
   },
   "source": [
    "# A more complex set of data\n",
    "\n",
    "That wasn't terribly interesting, so let us now define a bit more complex function to explore the complexity of the model. \n",
    "\n",
    "I put this into a function to later be able to modify this. I have chosen to use a type of function that is often used in introductions to this topic - a combination of a sinusoid and a linear increase, \n",
    "$$\n",
    "f(x) = \\sin(\\pi x) + \\frac{x}{3},\n",
    "$$\n",
    "which is defined in the function `true_func`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5wXi86baGrl3",
   "metadata": {
    "id": "5wXi86baGrl3"
   },
   "outputs": [],
   "source": [
    "def true_func(x):\n",
    "  \"\"\"The true function used by `make_fake_data`\"\"\"\n",
    "  return np.sin(x*np.pi)+x/3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CiN1w0CyHRYB",
   "metadata": {
    "id": "CiN1w0CyHRYB"
   },
   "outputs": [],
   "source": [
    "x, y = make_fake_data(25, true_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xh7Iye4yHThc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "xh7Iye4yHThc",
    "outputId": "1c28b735-a8b7-4cbb-9cbd-dc693aac5526"
   },
   "outputs": [],
   "source": [
    "show_data_func(x, y, true_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f11aa-b489-4523-baf6-cd0aefd2dc8d",
   "metadata": {
    "id": "5f4f11aa-b489-4523-baf6-cd0aefd2dc8d"
   },
   "source": [
    "# Fitting a seris of polynomials to the data.\n",
    "\n",
    "\n",
    "## Defining the model to use\n",
    "\n",
    "Any machine learning problem depends on specifying a model for the data. The choice of this model is problem specific and can strongly influence what you get out. Here we will use a particularly simple model - we will assume that our data are well represented by a polynomial:\n",
    "$$\n",
    "f(x) = \\sum_{i=0}^M a_i x^i.\n",
    "$$\n",
    "The *parameters* of this model are the $a_i$ and when we fit the polynominal with any of the algorithms used for that, we get estimates of these $a_i$. \n",
    "\n",
    "However, there is another \"parameter\" in this equation, the maximum order of the polynomial, $M$. In many real situations we might not have any knowledge of this parameter and each $M$ creates a new model. This parameter $M$ is known as a **hyper-parameter**. Hyper-parameters come in different guises in different ML methods - here it specifies the complexity of the model. \n",
    "\n",
    "We do not know the value of $M$, so first we will fit a series of polynomials and see which does best.\n",
    "\n",
    "The simplest way to assess the quality of the fit is to calculate the mean square error (MSE) and that is what we use below here.\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{N} \\sum_i (y_i - \\mathrm{model})^2\n",
    "$$\n",
    "\n",
    "so in the function `fit_polynomials_to_xy` below, we loop over the different orders we want to try, use `polyfit` to fit a polynomial to the given data and use this to calculate the MSE for each order, in the end the orders and MSEs are returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa073f1f-344a-4911-a2b3-5aef79ff8535",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa073f1f-344a-4911-a2b3-5aef79ff8535",
    "outputId": "a21cda50-d8df-4527-ffc3-82e099978508"
   },
   "outputs": [],
   "source": [
    "# We will fit up to an order of max_order with no default\n",
    "def fit_polynomials_to_xy(x, y, max_order=None):\n",
    "    \"\"\"Fit a polynomial to input x & y data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        x : numpy array\n",
    "            Input x-data\n",
    "        y : numpy array\n",
    "            Input y-data\n",
    "\n",
    "    Keywords\n",
    "    --------\n",
    "        max_order : int\n",
    "            Maximum polynomial order to consider. If not given \n",
    "            this is set to the number of elements in `x`.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        orders : numpy.array\n",
    "            The orders to consider\n",
    "        MSE : numpy.array\n",
    "            The mean square error\n",
    "        best_fit: list\n",
    "            A list with the polynomial coefficients of each fit.\n",
    "    \"\"\"\n",
    "    if max_order is None:\n",
    "        max_order = len(x)\n",
    "    \n",
    "    orders = np.arange(max_order)\n",
    "    MSE = np.zeros(max_order)\n",
    "\n",
    "    best_fit = []\n",
    "\n",
    "    for i, order in enumerate(orders):\n",
    "        # Fit the training sample using polyfit\n",
    "        p = np.polyfit(x, y, order)\n",
    "        best_fit.append(p)\n",
    "\n",
    "        # Calculate the best fit on the training sample\n",
    "        mu_fit_train = np.polyval(p, x)\n",
    "        MSE[i] = np.sum((y-mu_fit_train)**2)/len(x)\n",
    "\n",
    "    return orders, MSE, best_fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe2803-77e1-4d69-8b67-549581510947",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders, MSE, best_fit = fit_polynomials_to_xy(x, y, max_order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729f6fd-3a58-4116-b2b0-d56118c55806",
   "metadata": {},
   "source": [
    "## TASK: \n",
    "\n",
    "Explore what happens when you increase the maximum order - is there a natural maximum order to consider? Make a plot of how MSE varies with the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d6718-8bee-4992-8e6b-06c4b39609e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b60399fe-e60f-44c4-8fa2-61e5f641e0d4",
   "metadata": {},
   "source": [
    "# Summarising the polyfit\n",
    "\n",
    "I am now assuming that you have done the task above and have at least a maximum order $>13$ - otherwise the below will not work as planned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e2278-b958-4997-bcdb-6e2c8c5fe3f8",
   "metadata": {
    "id": "713e2278-b958-4997-bcdb-6e2c8c5fe3f8"
   },
   "outputs": [],
   "source": [
    "def get_extrema_and_index(x, y):\n",
    "    \"\"\"Find the minimum and maximum of a curve y_i=f(x_i) and return the indices as well\"\"\"\n",
    "\n",
    "    i_min = np.argmin(y)\n",
    "    i_max = np.argmax(y)\n",
    "\n",
    "    return i_min, i_max, x[i_min], x[i_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a80210-ba3d-4056-a435-9b4d82260276",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7a80210-ba3d-4056-a435-9b4d82260276",
    "outputId": "6fa64f4d-684a-4d2c-9063-205d8e88f1a6"
   },
   "outputs": [],
   "source": [
    "i_min_mse_train, i_max_mse_train, min_mse_train, max_mse_train = get_extrema_and_index(orders, MSE)\n",
    "print(\"The minimum MSE for the training sample happens for polynomial order {0}\".format(min_mse_train))\n",
    "print(\"The maximum MSE for the training sample happens for polynomial order {0}\".format(max_mse_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oiy9k8ioKJdv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "oiy9k8ioKJdv",
    "outputId": "11b94118-dc74-4e77-81b2-d956dc9505f4"
   },
   "outputs": [],
   "source": [
    "# We can now get the best-fitting function from the best_fit array.\n",
    "x_plot = np.linspace(0, 4, 200)\n",
    "p_best = best_fit[min_mse_train]\n",
    "y_fit = np.polyval(p_best, x_plot)\n",
    "fig, ax = plt.subplots(ncols=1, nrows=1)\n",
    "ax.plot(x_plot, y_fit, label='Best-fit polynomial', color='green')\n",
    "show_data_func(x, y, true_func, ax=ax, yrange=[-4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc4c53-382c-4e08-94b4-bb75ee2eae95",
   "metadata": {},
   "source": [
    "# Oops!\n",
    "\n",
    "This clearly does not seem to match our expectations - this does not seem to fit the data well at all.\n",
    "\n",
    "## We are now going back to the lecture for a bit."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
